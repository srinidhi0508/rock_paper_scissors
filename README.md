
# Rock Paper Scissors Detection 

This project uses a YOLOv11 object detection model trained on a custom dataset (from Roboflow) to detect hand gestures representing Rock, Paper, and Scissors.

## Tools & Libraries
- YOLOv11 via Ultralytics
- Google Colab for training
- Roboflow for dataset
- Python

##  Files
- `rock_paper_scissors.ipynb`: Full training notebook (Colab)
- `best.pt`: Trained YOLO model
- `sample_predictions/`: Detected images (Rock/Paper/Scissors)

##  How to Use
- Upload `best.pt` to your own YOLO project
- Use webcam or image detection code to detect gestures in real time

##  Future Work
- Real-time webcam detection
- Deploy as a web app

---

### Made by [Srinidhi vodnala]
